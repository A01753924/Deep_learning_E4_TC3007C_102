{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equipo 4:\n",
    "\n",
    "\n",
    "*   Karla Andrea Palma Villanueva (A01754270)\n",
    "*   Viviana Alanis Fraige (A01236316)\n",
    "* David Fernando Armendariz Torres (A01570813)\n",
    "* Alan Alberto Mota Yescas (A01753924)\n",
    "* Adrián Chávez Morales (A01568679)\n",
    "* Jose Manuel Armendáriz Mena (A01197583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "El objetivo de este notebook es implementar un modelo de Red Neuronal Convolucional (CNN) utilizando el dataset CIFAR-10 para la tarea de clasificación de imágenes. CIFAR-10 es un conjunto de datos ampliamente utilizado en el campo del aprendizaje profundo, que consta de 60,000 imágenes a color distribuidas en 10 categorías, tales como aviones, automóviles, pájaros y gatos. Cada imagen tiene un tamaño de 32x32 píxeles y está etiquetada con su respectiva clase.\n",
    "\n",
    "Además de desarrollar el modelo CNN, aplicaremos técnicas de Data Augmentation (aumento de datos) para incrementar la variedad del conjunto de entrenamiento. Esto nos permitirá reducir el riesgo de sobreajuste (overfitting) y mejorar la capacidad de generalización del modelo en datos no vistos.\n",
    "\n",
    "Al finalizar este trabajo, analizaremos el rendimiento del modelo evaluando métricas clave como la precisión (accuracy) y la pérdida (loss), comparando los resultados con y sin Data Augmentation. Nuestro objetivo final es demostrar que el uso de estas técnicas contribuye a un modelo más robusto y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración, explicación y limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Origen y Contexto del Dataset CIFAR-10\n",
    "\n",
    "El dataset CIFAR-10 fue desarrollado por Alex Krizhevsky, Geoffrey Hinton y Vinod Nair como parte del trabajo de investigación del grupo de Aprendizaje Profundo en la Universidad de Toronto. Es uno de los datasets más utilizados para experimentos en el campo de la visión por computadora debido a su simplicidad, pero al mismo tiempo, a su capacidad de ofrecer desafíos relevantes para la construcción de modelos robustos.\n",
    "\n",
    "Este conjunto de datos contiene 60,000 imágenes en color, de 32x32 píxeles cada una, distribuidas en 10 categorías diferentes:\n",
    "- Aviones\n",
    "- Automóviles\n",
    "- Pájaros\n",
    "- Gatos\n",
    "- Ciervos\n",
    "- Perros\n",
    "- Ranas\n",
    "- Caballos\n",
    "- Barcos\n",
    "- Camiones\n",
    "\n",
    "Cada categoría tiene 6,000 imágenes, divididas en 50,000 imágenes para entrenamiento y 10,000 imágenes para pruebas. Esta organización permite desarrollar y evaluar modelos de clasificación de imágenes con un tamaño de dataset razonable para tareas académicas o de investigación.\n",
    "\n",
    "El objetivo del CIFAR-10 es facilitar la experimentación en tareas de reconocimiento de patrones y clasificación de imágenes en situaciones donde las imágenes contienen objetos simples, pero con variaciones en poses, colores, y ángulos. Además, al ser un dataset equilibrado (con la misma cantidad de imágenes por clase), permite evaluar de manera precisa el desempeño de los modelos sin sesgos hacia una clase específica.\n",
    "\n",
    "La elección de este dataset es especialmente relevante en el campo del aprendizaje profundo, ya que se utiliza frecuentemente para enseñar conceptos fundamentales como Redes Neuronales Convolucionales (CNN) y técnicas de aumento de datos (Data Augmentation), las cuales son fundamentales para mejorar la generalización del modelo y prevenir el sobreajuste. \n",
    "\n",
    "CIFAR-10 es de acceso libre y está disponible en varios recursos de aprendizaje de TensorFlow y PyTorch, lo que facilita su uso para fines educativos y de investigación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar la semilla \n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 0us/step\n",
      "Tamaño del conjunto de entrenamiento: 50000\n",
      "Tamaño del conjunto de prueba: 10000\n",
      "Forma de las imágenes: (32, 32, 3)\n",
      "Cantidad de clases: 10\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalización del dataset \n",
    "mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {x_train.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {x_test.shape[0]}\")\n",
    "print(f\"Forma de las imágenes: {x_train.shape[1:]}\")\n",
    "print(f\"Cantidad de clases: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al cargar el dataset CIFAR-10, observamos las siguientes características:\n",
    "\n",
    "Conjunto de Entrenamiento:\n",
    "\n",
    "Tamaño: 50,000 imágenes\n",
    "Cada imagen tiene una forma de 32x32 píxeles y 3 canales de color (RGB), lo que implica que cada imagen es una matriz tridimensional de tamaño (32, 32, 3).\n",
    "Conjunto de Prueba:\n",
    "\n",
    "Tamaño: 10,000 imágenes\n",
    "Las imágenes tienen la misma estructura que las del conjunto de entrenamiento, es decir, (32, 32, 3).\n",
    "Etiquetas de Clase:\n",
    "El dataset contiene 10 categorías de imágenes diferentes. Las etiquetas se encuentran en los arrays y_train y y_test para los conjuntos de entrenamiento y prueba, respectivamente. Cada etiqueta es un número entero que representa una categoría específica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset CIFAR-10 no requiere procesos de limpieza adicionales, ya que se trata de un conjunto de datos ampliamente utilizado y cuidadosamente curado para tareas de aprendizaje profundo. Los datos se presentan de forma consistente, sin valores faltantes, duplicados o registros incompletos. Además, cada imagen está correctamente etiquetada y alineada con su categoría correspondiente. Esto garantiza que todas las observaciones estén listas para ser utilizadas en el modelo sin necesidad de aplicar procesos de limpieza convencionales, como la eliminación de valores nulos o el tratamiento de datos inconsistentes.\n",
    "\n",
    "Sin embargo, aunque la limpieza no es necesaria, sí realizamos transformaciones fundamentales para optimizar el rendimiento del modelo. La primera transformación aplicada fue la normalización de las imágenes, que consiste en restar la media y dividir por la desviación estándar de los valores de los píxeles en el conjunto de entrenamiento. Esto es crucial para asegurar que las intensidades de los píxeles, originalmente en un rango de 0 a 255, estén distribuidas en torno a una media de 0 con una desviación estándar de 1. La normalización mejora la eficiencia del entrenamiento al acelerar la convergencia del modelo y prevenir problemas de escala que podrían afectar la optimización de los parámetros.\n",
    "\n",
    "Otra transformación importante que implementaremos es el aumento de datos (Data Augmentation), una técnica que incrementa artificialmente la variedad del conjunto de entrenamiento. Esta técnica genera nuevas muestras a partir de las imágenes existentes mediante modificaciones como rotaciones aleatorias, traslaciones, inversiones horizontales, cambios en el brillo y la introducción de ruido. El Data Augmentation es esencial para evitar el sobreajuste del modelo, ya que ayuda a mejorar su capacidad de generalización al exponerlo a una mayor diversidad de patrones durante el entrenamiento. Esto permite que el modelo aprenda de manera más robusta y tenga un mejor desempeño al enfrentar nuevas imágenes no vistas. \n",
    "\n",
    "Estas transformaciones aseguran que los datos estén en condiciones óptimas para ser procesados por la Red Neuronal Convolucional, lo que facilita un entrenamiento eficiente y contribuye a construir un modelo con mayor capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo del Modelo de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación del modelo CNN en este proyecto sigue una arquitectura cuidadosamente diseñada para mejorar el rendimiento en la clasificación del dataset CIFAR-10. El modelo se compone de múltiples capas convolucionales organizadas en tres bloques, cada uno con dos capas convolucionales, seguidas de normalización por lotes (*Batch Normalization*), capas de agrupamiento máximo (*MaxPooling*) y capas de abandono (*Dropout*) para mitigar el sobreajuste. Estas decisiones permiten capturar características complejas de las imágenes y asegurar que el modelo pueda generalizar adecuadamente en datos no vistos. La adición de más capas en comparación con arquitecturas básicas de CNN contribuye a la creación de un modelo más profundo y robusto, capaz de aprender representaciones más complejas.\n",
    "\n",
    "La arquitectura del modelo se resume en tres bloques convolucionales progresivos que aumentan la profundidad del modelo, comenzando con 64 filtros en el primer bloque, 128 en el segundo y 256 en el tercero. Cada bloque termina con una capa de agrupamiento (*MaxPooling*) para reducir las dimensiones espaciales y una capa de abandono (*Dropout*) que ayuda a evitar el sobreajuste mediante la desactivación aleatoria de neuronas durante el entrenamiento. Tras los bloques convolucionales, se utiliza una capa de *Global Average Pooling* que reduce las dimensiones del espacio de características sin perder información relevante, seguida de una capa densa de 256 neuronas. Finalmente, la capa de salida tiene 10 neuronas con activación *softmax*, lo que permite que el modelo prediga la probabilidad de pertenencia a cada una de las 10 clases del dataset CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,217,354</span> (4.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,217,354\u001b[0m (4.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,215,562</span> (4.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,215,562\u001b[0m (4.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> (7.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,792\u001b[0m (7.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definición del modelo CNN \n",
    "def create_deeper_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Primera capa convolucional\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Segunda capa convolucional\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Tercera capa convolucional\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Capa de salida\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# modelo\n",
    "model = create_deeper_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(learning_rate=0.0005)\n",
    "# modelo usando SparseCategoricalCrossentropy\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el generador al conjunto de entrenamiento\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Definir los Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación del modelo CNN en este proyecto no solo se basa en una arquitectura profunda y bien estructurada, sino que también incluye diversas estrategias para optimizar el entrenamiento y mejorar el rendimiento general. La configuración del optimizador y la función de pérdida son fundamentales para garantizar una convergencia eficiente del modelo. Se utilizó el optimizador RMSprop con una tasa de aprendizaje de 0.0005, lo que ayuda a ajustar los pesos del modelo de manera efectiva en cada iteración. La función de pérdida seleccionada es Sparse Categorical Crossentropy, adecuada para tareas de clasificación con etiquetas codificadas como enteros. Además, se utiliza la métrica de precisión (*accuracy*) para evaluar el desempeño del modelo durante el entrenamiento y la validación.\n",
    "\n",
    "Un elemento clave en la implementación es la inclusión del Data Augmentation mediante la clase ImageDataGenerator, que genera variaciones aleatorias de las imágenes de entrenamiento. Esto incluye rotaciones, traslaciones horizontales y verticales, zoom, cizallamiento y volteo horizontal. Estas técnicas amplían artificialmente el conjunto de datos, ayudando a que el modelo aprenda patrones más diversos y evitando el sobreajuste. La integración del generador con el conjunto de entrenamiento se realiza mediante el método fit(), lo que permite que las transformaciones sean aplicadas en tiempo real durante el entrenamiento del modelo.\n",
    "\n",
    "Para mejorar aún más la eficiencia del proceso de entrenamiento, se han configurado callbacks que controlan la dinámica del entrenamiento. Entre estos se encuentra ReduceLROnPlateau, que reduce la tasa de aprendizaje si no se observa mejora en la pérdida de validación durante un número determinado de épocas, permitiendo que el modelo afine mejor los parámetros en las últimas etapas del entrenamiento. También se ha implementado EarlyStopping para detener el entrenamiento de manera anticipada si la precisión de validación deja de mejorar, evitando el uso innecesario de recursos computacionales y sobreentrenamiento. Finalmente, se añade un programador de la tasa de aprendizaje mediante LearningRateScheduler, que ajusta la tasa de aprendizaje en función del número de épocas, acelerando la convergencia en las primeras etapas y refinando el aprendizaje en las etapas finales. \n",
    "\n",
    "Estos componentes trabajan en conjunto para construir un modelo CNN eficiente, robusto y capaz de generalizar adecuadamente en tareas de clasificación de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 410ms/step - accuracy: 0.3344 - loss: 1.8741 - val_accuracy: 0.5381 - val_loss: 1.3789 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:47\u001b[0m 369ms/step - accuracy: 0.5000 - loss: 1.3376"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.5000 - loss: 1.3376 - val_accuracy: 0.5203 - val_loss: 1.4593 - learning_rate: 5.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 355ms/step - accuracy: 0.5469 - loss: 1.2790 - val_accuracy: 0.6085 - val_loss: 1.1443 - learning_rate: 5.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.5469 - loss: 1.0697 - val_accuracy: 0.5929 - val_loss: 1.2174 - learning_rate: 5.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 483ms/step - accuracy: 0.6172 - loss: 1.0973 - val_accuracy: 0.6828 - val_loss: 0.9173 - learning_rate: 5.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.5938 - loss: 1.0473 - val_accuracy: 0.6839 - val_loss: 0.9157 - learning_rate: 5.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 603ms/step - accuracy: 0.6653 - loss: 0.9697 - val_accuracy: 0.6860 - val_loss: 0.9520 - learning_rate: 5.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 30ms/step - accuracy: 0.6719 - loss: 1.1266 - val_accuracy: 0.7343 - val_loss: 0.7648 - learning_rate: 5.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 589ms/step - accuracy: 0.6952 - loss: 0.8847 - val_accuracy: 0.7450 - val_loss: 0.7378 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 0.6440 - val_accuracy: 0.7460 - val_loss: 0.7606 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 379ms/step - accuracy: 0.7179 - loss: 0.8309 - val_accuracy: 0.7689 - val_loss: 0.7009 - learning_rate: 5.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.7812 - loss: 0.6793 - val_accuracy: 0.7606 - val_loss: 0.7374 - learning_rate: 5.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 387ms/step - accuracy: 0.7344 - loss: 0.7785 - val_accuracy: 0.7365 - val_loss: 0.8140 - learning_rate: 5.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:17\u001b[0m 408ms/step - accuracy: 0.7500 - loss: 0.8437\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.7500 - loss: 0.8437 - val_accuracy: 0.7568 - val_loss: 0.7289 - learning_rate: 2.5000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 387ms/step - accuracy: 0.7584 - loss: 0.7048 - val_accuracy: 0.7866 - val_loss: 0.6385 - learning_rate: 2.5000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.7344 - loss: 0.7215 - val_accuracy: 0.7860 - val_loss: 0.6489 - learning_rate: 2.5000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 382ms/step - accuracy: 0.7735 - loss: 0.6649 - val_accuracy: 0.8071 - val_loss: 0.5972 - learning_rate: 2.5000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.8125 - loss: 0.5223 - val_accuracy: 0.8093 - val_loss: 0.5956 - learning_rate: 2.5000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 388ms/step - accuracy: 0.7813 - loss: 0.6434 - val_accuracy: 0.8181 - val_loss: 0.5386 - learning_rate: 2.5000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.7188 - loss: 1.0119 - val_accuracy: 0.8215 - val_loss: 0.5332 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    validation_data=(x_test, y_test),\n",
    "    steps_per_epoch=x_train.shape[0] // 64,\n",
    "    epochs=20,\n",
    "    callbacks=[reduce_lr, early_stopping, lr_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos durante el entrenamiento del modelo muestran un progreso constante en la precisión tanto del conjunto de entrenamiento como del conjunto de validación a lo largo de las 20 épocas. Al inicio del entrenamiento, la precisión en el conjunto de entrenamiento es baja (50%) y la pérdida es alta (1.3376), lo que es esperado al tratarse de las primeras iteraciones del modelo en la tarea de clasificación. A medida que avanzan las épocas, el modelo mejora progresivamente en ambas métricas. Por ejemplo, hacia la época 10, la precisión del conjunto de validación alcanza 74.5% y la pérdida disminuye a 0.7378, lo que indica que el modelo está aprendiendo patrones significativos de los datos.\n",
    "\n",
    "Un aspecto notable del entrenamiento es la activación del callback ReduceLROnPlateau en la época 14. Este mecanismo reduce la tasa de aprendizaje a la mitad, lo que permite al modelo ajustar los pesos de manera más fina en las últimas etapas del entrenamiento, evitando grandes fluctuaciones en la pérdida. Después de este ajuste, la precisión del conjunto de validación sigue mejorando, alcanzando 82.15% en la última época con una pérdida de 0.5332. Esto sugiere que el modelo fue capaz de generalizar bien sin caer en problemas de sobreajuste, dado que las diferencias entre la precisión del entrenamiento y la validación son relativamente pequeñas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 15s - 48ms/step - accuracy: 0.8215 - loss: 0.5332\n",
      "\n",
      "Test accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "xaxis": "x",
         "y": [
          1.6247798204421997,
          1.3376126289367676,
          1.2237526178359985,
          1.0696709156036377,
          1.0655852556228638,
          1.0472655296325684,
          0.9482751488685608,
          1.1266319751739502,
          0.8871313333511353,
          0.6440147757530212,
          0.8266251683235168,
          0.6793444156646729,
          0.782012403011322,
          0.8436943292617798,
          0.694718599319458,
          0.7215052843093872,
          0.667169988155365,
          0.522300124168396,
          0.6482835412025452,
          1.011926531791687
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Val Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "xaxis": "x",
         "y": [
          1.3788642883300781,
          1.4593098163604736,
          1.1442993879318237,
          1.217415452003479,
          0.9173258543014526,
          0.9156665205955505,
          0.9520339965820312,
          0.7647641897201538,
          0.7377941608428955,
          0.7606004476547241,
          0.700867235660553,
          0.7374408841133118,
          0.8140044212341309,
          0.7288820147514343,
          0.63847416639328,
          0.648853063583374,
          0.5971565246582031,
          0.5955682396888733,
          0.5385663509368896,
          0.5332251191139221
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "xaxis": "x2",
         "y": [
          0.41635292768478394,
          0.5,
          0.5702298879623413,
          0.546875,
          0.6287247538566589,
          0.59375,
          0.6728813052177429,
          0.671875,
          0.6964314579963684,
          0.78125,
          0.717738687992096,
          0.78125,
          0.7331584692001343,
          0.75,
          0.7624359130859375,
          0.734375,
          0.7729493975639343,
          0.8125,
          0.7810196876525879,
          0.71875
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines+markers",
         "name": "Val Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "xaxis": "x2",
         "y": [
          0.538100004196167,
          0.5202999711036682,
          0.6085000038146973,
          0.5928999781608582,
          0.6827999949455261,
          0.683899998664856,
          0.6859999895095825,
          0.7343000173568726,
          0.7450000047683716,
          0.7459999918937683,
          0.7688999772071838,
          0.7605999708175659,
          0.7365000247955322,
          0.7567999958992004,
          0.7865999937057495,
          0.7860000133514404,
          0.8070999979972839,
          0.8093000054359436,
          0.8180999755859375,
          0.8215000033378601
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Pérdida durante el entrenamiento",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Precisión durante el entrenamiento",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 400,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Historia del Entrenamiento"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Graficar la historia del entrenamiento\n",
    "def plot_history(history):\n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                        subplot_titles=('Pérdida durante el entrenamiento', \n",
    "                                        'Precisión durante el entrenamiento'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history['loss']))),\n",
    "        y=history.history['loss'],\n",
    "        mode='lines+markers',\n",
    "        name='Loss'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history['val_loss']))),\n",
    "        y=history.history['val_loss'],\n",
    "        mode='lines+markers',\n",
    "        name='Val Loss'\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history['accuracy']))),\n",
    "        y=history.history['accuracy'],\n",
    "        mode='lines+markers',\n",
    "        name='Accuracy'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history.history['val_accuracy']))),\n",
    "        y=history.history['val_accuracy'],\n",
    "        mode='lines+markers',\n",
    "        name='Val Accuracy'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text='Historia del Entrenamiento',\n",
    "        height=400, width=800\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados e Interpretación 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las gráficas presentadas ilustran la evolución de las métricas de pérdida y precisión a lo largo del entrenamiento del modelo CNN, tanto para el conjunto de entrenamiento como para el de validación. Estas visualizaciones son fundamentales para evaluar el rendimiento del modelo y verificar si está aprendiendo de manera efectiva sin incurrir en problemas de sobreajuste.\n",
    "\n",
    "En la gráfica de pérdida durante el entrenamiento (a la izquierda), observamos cómo la pérdida disminuye progresivamente en ambas curvas, tanto para el conjunto de entrenamiento como para el conjunto de validación. Esto indica que el modelo está aprendiendo patrones significativos de los datos y mejorando en la minimización de los errores de predicción. Sin embargo, al final del proceso (épocas 19-20), la curva de pérdida para el conjunto de validación muestra un ligero aumento, lo que podría ser un indicio de que el modelo comienza a sobreajustarse. Esta situación es importante monitorearla para evitar que el modelo pierda capacidad de generalización.\n",
    "\n",
    "Por otro lado, la gráfica de precisión durante el entrenamiento (a la derecha) muestra un incremento constante en la precisión tanto del conjunto de entrenamiento como del conjunto de validación. Desde un valor inicial cercano al 50%, la precisión de validación alcanza aproximadamente 82% en la última época, lo que indica un desempeño sólido. A lo largo de las primeras 10 épocas, se observa un crecimiento acelerado en la precisión, lo que sugiere que el modelo ha encontrado rápidamente patrones útiles. Después de la época 15, el crecimiento se estabiliza, mostrando que el modelo está refinando los últimos ajustes.\n",
    "\n",
    "En general, ambas gráficas muestran una convergencia adecuada, con mejoras continuas en las métricas de entrenamiento y validación. El uso de callbacks, como ReduceLROnPlateau y EarlyStopping, ha permitido que el modelo ajuste sus parámetros de manera más eficiente. Aunque las curvas de validación no presentan grandes fluctuaciones, el ligero aumento en la pérdida al final del entrenamiento sugiere que podría explorarse una reducción adicional en la tasa de aprendizaje o el uso de más datos para evitar sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aplicación de ajustes de hiperparámetros en un modelo CNN es fundamental para mejorar su rendimiento y optimizar su capacidad de generalización. En este proyecto, se ha implementado un proceso de **sintonización automática de hiperparámetros utilizando keras_tuner, lo que permite explorar múltiples configuraciones del modelo y seleccionar aquellas que proporcionan los mejores resultados en términos de precisión en el conjunto de validación. La importancia de estos ajustes radica en que los hiperparámetros, como el número de filtros en las capas convolucionales, el tamaño de las capas densas, las tasas de aprendizaje, y el dropout, afectan directamente la capacidad del modelo para aprender patrones significativos sin sobreajustarse a los datos de entrenamiento.\n",
    "\n",
    "En este modelo, se han utilizado varias configuraciones dinámicas, como la búsqueda del número óptimo de filtros en las capas convolucionales (variando entre 32 y 256) y el ajuste de las unidades de la capa densa final. Además, la inclusión de Data Augmentation con variaciones en rotación, desplazamiento, zoom y brillo contribuye a enriquecer el conjunto de datos y a mejorar la capacidad de generalización del modelo. El uso de learning rate adaptable permite que el optimizador Adam ajuste los pesos de manera más precisa durante el proceso de entrenamiento, evitando que el modelo converja demasiado rápido hacia un mínimo local.\n",
    "\n",
    "Otro ajuste importante es la integración de un modelo preentrenado, ResNet50, para aprovechar los pesos de una red profunda entrenada en el dataset ImageNet. Esta técnica de transfer learning permite reutilizar características previamente aprendidas en un nuevo contexto, lo que acelera el proceso de entrenamiento y mejora la precisión. Al congelar las capas de la red base, el modelo se enfoca en aprender patrones específicos del dataset CIFAR-10 sin modificar las características generales ya adquiridas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\cifar10_tuning\\tuner0.json\n",
      "Mejores hiperparámetros: {'filters_1': 96, 'filters_2': 192, 'units': 192, 'dropout': 0.2, 'learning_rate': 0.002312113349326375}\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=32, max_value=128, step=32)\n",
    "    model.add(Conv2D(hp_filters_1, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "    hp_filters_2 = hp.Int('filters_2', min_value=64, max_value=256, step=64)\n",
    "    model.add(Conv2D(hp_filters_2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=64, max_value=256, step=64)\n",
    "    model.add(Dense(hp_units, activation='relu'))\n",
    "\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,  \n",
    "    executions_per_trial=1,  \n",
    "    directory='my_dir',\n",
    "    project_name='cifar10_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(datagen.flow(x_train, y_train, batch_size=64),\n",
    "             validation_data=(x_test, y_test),\n",
    "             steps_per_epoch=x_train.shape[0] // 64,\n",
    "             epochs=10)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Mejores hiperparámetros: {best_hps.values}\")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=64, max_value=128, step=32)\n",
    "    model.add(Conv2D(hp_filters_1, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    hp_filters_2 = hp.Int('filters_2', min_value=64, max_value=256, step=64)\n",
    "    model.add(Conv2D(hp_filters_2, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    hp_units = hp.Int('units', min_value=128, max_value=256, step=64)\n",
    "    model.add(Dense(hp_units, activation='relu'))\n",
    "    model.add(Dropout(0.3)) \n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-3, max_value=1e-2, sampling='log')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "base_model.trainable = False  \n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 470ms/step - accuracy: 0.1116 - loss: 2.3674 - val_accuracy: 0.1717 - val_loss: 2.1957\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 528ms/step - accuracy: 0.1397 - loss: 2.2196 - val_accuracy: 0.2173 - val_loss: 2.1469\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 535ms/step - accuracy: 0.1521 - loss: 2.1940 - val_accuracy: 0.2293 - val_loss: 2.1246\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 529ms/step - accuracy: 0.1524 - loss: 2.1863 - val_accuracy: 0.2490 - val_loss: 2.0795\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 507ms/step - accuracy: 0.1573 - loss: 2.1748 - val_accuracy: 0.2344 - val_loss: 2.0862\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 549ms/step - accuracy: 0.1619 - loss: 2.1611 - val_accuracy: 0.2463 - val_loss: 2.0455\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 526ms/step - accuracy: 0.1629 - loss: 2.1591 - val_accuracy: 0.2510 - val_loss: 2.0189\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 497ms/step - accuracy: 0.1677 - loss: 2.1531 - val_accuracy: 0.1994 - val_loss: 2.1052\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 473ms/step - accuracy: 0.1667 - loss: 2.1506 - val_accuracy: 0.2545 - val_loss: 2.0622\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 515ms/step - accuracy: 0.1665 - loss: 2.1531 - val_accuracy: 0.2471 - val_loss: 2.0328\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=64,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados del entrenamiento mostrados reflejan que la configuración del modelo no fue la más adecuada, lo que afectó significativamente el desempeño y la eficiencia del proceso. A pesar de completar las 10 épocas de entrenamiento, los valores de precisión tanto en el conjunto de entrenamiento como en el de validación se mantuvieron extremadamente bajos, con una precisión final de 16.65% en el entrenamiento y 24.71% en la validación. Estos resultados indican que el modelo no fue capaz de aprender correctamente los patrones presentes en los datos, sugiriendo que la arquitectura y los hiperparámetros seleccionados no lograron capturar la complejidad del problema.\n",
    "\n",
    "Además, la pérdida elevada en ambas curvas confirma que el modelo no logró optimizar de manera efectiva las predicciones, lo que podría indicar un mal ajuste en los hiperparámetros, como el número de filtros, unidades densas, o la tasa de aprendizaje. También es posible que la arquitectura del modelo no sea lo suficientemente profunda o compleja para este tipo de tarea, o que los datos no se hayan preprocesado y utilizado de forma adecuada. \n",
    "\n",
    "Otro problema destacado es el tiempo excesivo de entrenamiento, con un promedio de más de 4 minutos por época, lo que resulta ineficiente para obtener resultados subóptimos. Este tiempo prolongado sugiere que el modelo podría estar sobredimensionado en relación al problema o que se están utilizando configuraciones innecesariamente costosas en términos de recursos computacionales, como un tamaño de lote o arquitectura no ajustados.\n",
    "\n",
    "Dado el rendimiento limitado y la larga duración del entrenamiento, se recomienda una revisión completa de la configuración del modelo. Esto incluiría:\n",
    "- Ajustar el tamaño de las capas convolucionales y densas.\n",
    "- Experimentar con tasas de aprendizaje más bajas o utilizando estrategias adaptativas como ReduceLROnPlateau.\n",
    "- Aumentar las épocas solo si se obtiene una mejor convergencia con los ajustes realizados.\n",
    "- Considerar arquitecturas preentrenadas (como ResNet o MobileNet) para mejorar el rendimiento mediante transfer learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          2.2849979400634766,
          2.2122256755828857,
          2.1915454864501953,
          2.1774370670318604,
          2.1692612171173096,
          2.1604909896850586,
          2.157489538192749,
          2.1508021354675293,
          2.1523566246032715,
          2.146512985229492
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Val Loss",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x",
         "y": [
          2.1956582069396973,
          2.1469223499298096,
          2.1245877742767334,
          2.079496383666992,
          2.086176872253418,
          2.0454604625701904,
          2.018887996673584,
          2.105236768722534,
          2.0622470378875732,
          2.0328056812286377
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines+markers",
         "name": "Train Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x2",
         "y": [
          0.12082000076770782,
          0.14219999313354492,
          0.15265999734401703,
          0.1551399976015091,
          0.1609800010919571,
          0.1635800004005432,
          0.1656399965286255,
          0.16872000694274902,
          0.1651799976825714,
          0.16886000335216522
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines+markers",
         "name": "Val Accuracy",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
         ],
         "xaxis": "x2",
         "y": [
          0.17170000076293945,
          0.21729999780654907,
          0.22930000722408295,
          0.24899999797344208,
          0.23440000414848328,
          0.24629999697208405,
          0.25099998712539673,
          0.19939999282360077,
          0.25450000166893005,
          0.24709999561309814
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Pérdida (Loss)",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Precisión (Accuracy)",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 500,
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Curvas de Pérdida y Precisión del Modelo"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "Épocas"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Valor"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Pérdida (Loss)\", \"Precisión (Accuracy)\"))\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=train_loss, mode='lines+markers', name='Train Loss'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=val_loss, mode='lines+markers', name='Val Loss'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=train_acc, mode='lines+markers', name='Train Accuracy'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=list(epochs), y=val_acc, mode='lines+markers', name='Val Accuracy'), row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Curvas de Pérdida y Precisión del Modelo\",\n",
    "    xaxis_title=\"Épocas\",\n",
    "    yaxis_title=\"Valor\",\n",
    "    showlegend=True,\n",
    "    height=500, width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados e Interpretación 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las gráficas de pérdida y precisión reflejan claramente los problemas identificados durante el entrenamiento del modelo. En la primera gráfica, correspondiente a la pérdida (Loss), se observa una disminución progresiva tanto en el conjunto de entrenamiento como en el conjunto de validación. Sin embargo, las pérdidas se mantienen en valores relativamente altos, alrededor de 2.1, lo que sugiere que el modelo no logró optimizar correctamente los pesos para mejorar las predicciones. Esto indica que el aprendizaje del modelo es insuficiente y que no está capturando de manera efectiva los patrones presentes en los datos.\n",
    "\n",
    "En la segunda gráfica, que muestra la precisión (Accuracy), se aprecia un crecimiento muy lento en ambas curvas. La precisión en el conjunto de entrenamiento apenas llega a 16.5% después de 10 épocas, mientras que la precisión en el conjunto de validación muestra ligeras fluctuaciones, alcanzando un máximo de 25.4%. Estas cifras indican que el modelo no está aprendiendo adecuadamente y que la arquitectura actual no es capaz de generalizar bien en los datos de validación. Además, la oscilación en la precisión del conjunto de validación refleja que el modelo está teniendo dificultades para estabilizar el aprendizaje, posiblemente debido a una configuración subóptima de los hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este proyecto, se llevó a cabo el desarrollo de un modelo de Red Neuronal Convolucional (CNN) para la clasificación de imágenes utilizando el dataset CIFAR-10. A lo largo del proceso, se realizaron diversos experimentos y ajustes, incluyendo la implementación de Data Augmentation, la sintonización de hiperparámetros con keras_tuner, y el uso de técnicas avanzadas como transfer learning con ResNet50. Sin embargo, los resultados obtenidos evidencian que la configuración inicial del modelo no fue óptima, lo que afectó significativamente su rendimiento, como se refleja en las bajas precisiones tanto en el conjunto de entrenamiento como en el de validación, junto con pérdidas elevadas que indican dificultades en la optimización del aprendizaje.\n",
    "\n",
    "El análisis de las métricas y las curvas de pérdida y precisión mostró que, a pesar de los esfuerzos por mejorar el modelo, los ajustes realizados no fueron suficientes para capturar de manera eficiente los patrones presentes en los datos. Además, el proceso de entrenamiento resultó ser computacionalmente costoso, con tiempos excesivos por época y sin una mejora significativa en el desempeño. Esto sugiere que la arquitectura utilizada y los hiperparámetros seleccionados no fueron adecuados para resolver el problema de clasificación de manera efectiva.\n",
    "\n",
    "A pesar de los desafíos enfrentados, este proyecto ofrece valiosos aprendizajes sobre la importancia de ajustar correctamente las configuraciones del modelo y seleccionar las arquitecturas apropiadas para cada tarea. La experiencia adquirida destaca la necesidad de iterar sobre las configuraciones del modelo, probar diferentes enfoques y ajustar los hiperparámetros de manera más precisa. En futuros trabajos, se recomienda explorar arquitecturas más profundas, realizar una validación más exhaustiva de los hiperparámetros, y utilizar modelos preentrenados para acelerar el aprendizaje y mejorar la precisión. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
